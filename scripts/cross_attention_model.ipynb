{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13930604,"sourceType":"datasetVersion","datasetId":8877392},{"sourceId":13978772,"sourceType":"datasetVersion","datasetId":8911333}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\nradmilasegen_dataset_path = kagglehub.dataset_download('radmilasegen/dataset')\n\nprint('Data source import complete.')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3Z5cyXlKrXj","outputId":"0cfa0bee-73ec-411a-fd1e-1fa5d0c0200b","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.241101Z","iopub.execute_input":"2025-12-06T21:20:03.241437Z","iopub.status.idle":"2025-12-06T21:20:03.865086Z","shell.execute_reply.started":"2025-12-06T21:20:03.241414Z","shell.execute_reply":"2025-12-06T21:20:03.864452Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nfrom glob import glob\n\n# BASE_PATH = \"/root/.cache/kagglehub/datasets/radmilasegen/dataset/versions/1/new_ds\"\nBASE_PATH = \"/kaggle/input/dataset/new_ds\"\nCLASSES_PATH = os.path.join(BASE_PATH, \"merged_classes.txt\")","metadata":{"id":"WyMJWxP4K1tT","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.866173Z","iopub.execute_input":"2025-12-06T21:20:03.866401Z","iopub.status.idle":"2025-12-06T21:20:03.870198Z","shell.execute_reply.started":"2025-12-06T21:20:03.866385Z","shell.execute_reply":"2025-12-06T21:20:03.869588Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.resnet import resnet50\nfrom torchvision.ops.feature_pyramid_network import (\n    FeaturePyramidNetwork,\n    LastLevelMaxPool,\n)\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import MultiScaleRoIAlign\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.870778Z","iopub.execute_input":"2025-12-06T21:20:03.871017Z","iopub.status.idle":"2025-12-06T21:20:03.890188Z","shell.execute_reply.started":"2025-12-06T21:20:03.870996Z","shell.execute_reply":"2025-12-06T21:20:03.889412Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"IMG_SIZE = 256\nEPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.891813Z","iopub.execute_input":"2025-12-06T21:20:03.892023Z","iopub.status.idle":"2025-12-06T21:20:03.904861Z","shell.execute_reply.started":"2025-12-06T21:20:03.891998Z","shell.execute_reply":"2025-12-06T21:20:03.904155Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def load_classes(path=CLASSES_PATH):\n    with open(path, \"r\") as f:\n        return [line.strip() for line in f.readlines() if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.905494Z","iopub.execute_input":"2025-12-06T21:20:03.905693Z","iopub.status.idle":"2025-12-06T21:20:03.920368Z","shell.execute_reply.started":"2025-12-06T21:20:03.905677Z","shell.execute_reply":"2025-12-06T21:20:03.919773Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def read_label(label_path):\n    objects = []\n    with open(label_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != 5:\n                continue\n            cls_id = int(parts[0])\n            x, y, w, h = map(float, parts[1:])\n            objects.append(\n                {\n                    \"cls_id\": cls_id,\n                    \"class_name\": CLASSES[cls_id],\n                    \"x_center\": x,\n                    \"y_center\": y,\n                    \"width\": w,\n                    \"height\": h,\n                }\n            )\n    return objects","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.921123Z","iopub.execute_input":"2025-12-06T21:20:03.921326Z","iopub.status.idle":"2025-12-06T21:20:03.935364Z","shell.execute_reply.started":"2025-12-06T21:20:03.921307Z","shell.execute_reply":"2025-12-06T21:20:03.934702Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def collect_samples(base_path=BASE_PATH):\n    samples = []\n    subfolders = sorted([f for f in os.listdir(base_path) if f.isdigit()])\n\n    for folder in subfolders:\n        img_dir = os.path.join(base_path, folder, \"images\")\n        depth_dir = os.path.join(base_path, folder, \"depth\")\n        label_dir = os.path.join(base_path, folder, \"labels\")\n\n        if not (\n            os.path.isdir(img_dir)\n            and os.path.isdir(depth_dir)\n            and os.path.isdir(label_dir)\n        ):\n            continue\n\n        img_files = sorted(glob(os.path.join(img_dir, \"*.png\")))\n\n        for img_path in img_files:\n            fname = os.path.basename(img_path)\n            depth_path = os.path.join(depth_dir, fname)\n            label_path = os.path.join(label_dir, fname.replace(\".png\", \".txt\"))\n\n            if not os.path.exists(depth_path):\n                continue\n            if not os.path.exists(label_path):\n                continue\n\n            objs = read_label(label_path)\n            if len(objs) == 0:\n                continue\n\n            samples.append(\n                {\n                    \"rgb\": img_path,\n                    \"depth\": depth_path,\n                    \"label\": label_path,\n                    \"objects\": objs,\n                }\n            )\n    return samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.936087Z","iopub.execute_input":"2025-12-06T21:20:03.936367Z","iopub.status.idle":"2025-12-06T21:20:03.951760Z","shell.execute_reply.started":"2025-12-06T21:20:03.936324Z","shell.execute_reply":"2025-12-06T21:20:03.951041Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def compute_depth_min_max(samples):\n    d_min = float(\"inf\")\n    d_max = float(\"-inf\")\n    for s in samples:\n        depth = cv2.imread(s[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        if depth.size == 0:\n            continue\n        m = depth.min()\n        M = depth.max()\n        if M <= 0:\n            continue\n        d_min = min(d_min, m)\n        d_max = max(d_max, M)\n\n    if not np.isfinite(d_min):\n        d_min = 0.0\n    if not np.isfinite(d_max) or d_max <= d_min:\n        d_max = d_min + 1.0\n    return d_min, d_max","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.952507Z","iopub.execute_input":"2025-12-06T21:20:03.952851Z","iopub.status.idle":"2025-12-06T21:20:03.969468Z","shell.execute_reply.started":"2025-12-06T21:20:03.952829Z","shell.execute_reply":"2025-12-06T21:20:03.968780Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class RGBDDetectionDataset(Dataset):\n    def __init__(self, samples, img_size=IMG_SIZE):\n        self.samples = samples\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n\n        rgb = cv2.imread(item[\"rgb\"])\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        rgb = cv2.resize(rgb, (self.img_size, self.img_size))\n        rgb = rgb.astype(np.float32)\n\n        depth = cv2.imread(item[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        depth = cv2.resize(depth, (self.img_size, self.img_size))\n\n        d = np.clip(depth, DEPTH_MIN, DEPTH_MAX)\n        d_norm = (d - DEPTH_MIN) / (DEPTH_MAX - DEPTH_MIN + 1e-6) \n\n        depth_ch = d_norm[..., None] \n\n        rgbd = np.concatenate([rgb / 255.0, depth_ch], axis=2) \n        rgbd = torch.from_numpy(rgbd).permute(2, 0, 1) \n\n        boxes = []\n        labels = []\n        for obj in item[\"objects\"]:\n            cx = obj[\"x_center\"]\n            cy = obj[\"y_center\"]\n            w = obj[\"width\"]\n            h = obj[\"height\"]\n\n            x1 = (cx - w / 2.0) * self.img_size\n            y1 = (cy - h / 2.0) * self.img_size\n            x2 = (cx + w / 2.0) * self.img_size\n            y2 = (cy + h / 2.0) * self.img_size\n\n            x1 = np.clip(x1, 0, self.img_size - 1)\n            y1 = np.clip(y1, 0, self.img_size - 1)\n            x2 = np.clip(x2, 0, self.img_size - 1)\n            y2 = np.clip(y2, 0, self.img_size - 1)\n\n            boxes.append([x1, y1, x2, y2])\n            labels.append(obj[\"cls_id\"] + 1)\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([idx])}\n\n        return rgbd, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.970217Z","iopub.execute_input":"2025-12-06T21:20:03.970396Z","iopub.status.idle":"2025-12-06T21:20:03.982112Z","shell.execute_reply.started":"2025-12-06T21:20:03.970382Z","shell.execute_reply":"2025-12-06T21:20:03.981335Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def collate_fn(batch):\n    images, targets = list(zip(*batch))\n    return list(images), list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:03.983893Z","iopub.execute_input":"2025-12-06T21:20:03.984276Z","iopub.status.idle":"2025-12-06T21:20:03.999933Z","shell.execute_reply.started":"2025-12-06T21:20:03.984253Z","shell.execute_reply":"2025-12-06T21:20:03.999236Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class ConvStem(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.stem(x) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:04.000639Z","iopub.execute_input":"2025-12-06T21:20:04.000955Z","iopub.status.idle":"2025-12-06T21:20:04.014971Z","shell.execute_reply.started":"2025-12-06T21:20:04.000937Z","shell.execute_reply":"2025-12-06T21:20:04.014341Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class CrossModalAttention(nn.Module):\n    def __init__(self, channels=64, attn_size=8):\n        super().__init__()\n        self.channels = channels\n        self.attn_size = attn_size\n\n        self.pool_rgb = nn.AdaptiveAvgPool2d((attn_size, attn_size))\n        self.pool_d = nn.AdaptiveAvgPool2d((attn_size, attn_size))\n\n        self.q_rgb = nn.Conv2d(channels, channels, 1)\n        self.k_d = nn.Conv2d(channels, channels, 1)\n        self.v_d = nn.Conv2d(channels, channels, 1)\n\n        self.alpha = nn.Parameter(torch.tensor(0.5))\n\n    def forward(self, F_rgb, F_d):\n        B, C, H, W = F_rgb.shape\n\n        R = self.pool_rgb(F_rgb) \n        D = self.pool_d(F_d) \n\n        Q = self.q_rgb(R)\n        K = self.k_d(D)\n        V = self.v_d(D)\n\n        B, C, h, w = Q.shape\n        T = h * w \n\n        Qf = Q.view(B, C, T)\n        Kf = K.view(B, C, T)\n        Vf = V.view(B, C, T)\n\n        A = torch.bmm(Qf.transpose(1, 2), Kf)\n        A = torch.softmax(A, dim=-1)\n\n        F_att_flat = torch.bmm(Vf, A.transpose(1, 2))\n        F_att_small = F_att_flat.view(B, C, h, w)\n\n        F_att = F.interpolate(F_att_small, size=(H, W), mode=\"bilinear\", align_corners=False)\n\n        fused = self.alpha * F_att + (1.0 - self.alpha) * F_rgb\n        return fused","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:04.015782Z","iopub.execute_input":"2025-12-06T21:20:04.016265Z","iopub.status.idle":"2025-12-06T21:20:04.032956Z","shell.execute_reply.started":"2025-12-06T21:20:04.016247Z","shell.execute_reply":"2025-12-06T21:20:04.032281Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class RGBD_Backbone(nn.Module):\n    def __init__(self, trainable_layers=5):\n        super().__init__()\n\n        self.rgb_stem = ConvStem(in_channels=3)\n        self.depth_stem = ConvStem(in_channels=1)\n        self.fusion = CrossModalAttention(channels=64, attn_size=8)\n        self.body = resnet50(weights=None)\n\n        self.body.conv1 = nn.Conv2d(\n            64,\n            64,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False,\n        )\n\n        layers_to_train = [\"layer2\", \"layer3\", \"layer4\"]\n        for name, param in self.body.named_parameters():\n            if not any([layer in name for layer in layers_to_train]):\n                param.requires_grad = False\n\n        self.out_channels = 256\n        self.fpn = FeaturePyramidNetwork(\n            in_channels_list=[ 256, 512, 1024, 2048,],\n            out_channels=self.out_channels,\n            extra_blocks=LastLevelMaxPool(),)\n        \n    def forward(self, x):\n        rgb = x[:, :3, :, :]\n        depth = x[:, 3:, :, :]\n\n        F_rgb = self.rgb_stem(rgb)\n        F_d = self.depth_stem(depth)\n        \n        F_fused = self.fusion(F_rgb, F_d)\n\n        x = self.body.conv1(F_fused)\n        x = self.body.bn1(x)\n        x = self.body.relu(x)\n        x = self.body.maxpool(x)\n\n        C2 = self.body.layer1(x)\n        C3 = self.body.layer2(C2)\n        C4 = self.body.layer3(C3)\n        C5 = self.body.layer4(C4)\n\n        feats = self.fpn({\"0\": C2, \"1\": C3, \"2\": C4, \"3\": C5})\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:04.033761Z","iopub.execute_input":"2025-12-06T21:20:04.034088Z","iopub.status.idle":"2025-12-06T21:20:04.049490Z","shell.execute_reply.started":"2025-12-06T21:20:04.034064Z","shell.execute_reply":"2025-12-06T21:20:04.048967Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def show_predictions(model, samples, score_thresh=0.5):\n    model.eval()\n    chosen = random.sample(samples, min(5, len(samples)))\n\n    for idx, sample in enumerate(chosen):\n        print(f\"\\n============= SAMPLE {idx+1} =============\")\n\n        # читаем RGB/DEPTH так же, как в датасете\n        rgb = cv2.imread(sample[\"rgb\"])\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE))\n        rgb_vis = rgb.copy()\n\n        depth = cv2.imread(sample[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        depth = cv2.resize(depth, (IMG_SIZE, IMG_SIZE))\n        d = np.clip(depth, DEPTH_MIN, DEPTH_MAX)\n        d_norm = (d - DEPTH_MIN) / (DEPTH_MAX - DEPTH_MIN + 1e-6)\n\n        depth_ch = d_norm[..., None]  # [0,1]\n\n        rgbd = np.concatenate([rgb.astype(np.float32) / 255.0, depth_ch], axis=2)\n        rgbd_t = torch.from_numpy(rgbd).permute(2, 0, 1).float().to(device)\n\n        with torch.no_grad():\n            outputs = model([rgbd_t])[0]\n\n        boxes = outputs[\"boxes\"].cpu().numpy()\n        labels = outputs[\"labels\"].cpu().numpy()\n        scores = outputs[\"scores\"].cpu().numpy()\n\n        for box, label, score in zip(boxes, labels, scores):\n            if score < score_thresh:\n                continue\n            x1, y1, x2, y2 = box.astype(int)\n            cls_id = int(label) - 1  # back to 0..C-1\n            if cls_id < 0 or cls_id >= len(CLASSES):\n                continue\n            cls_name = CLASSES[cls_id]\n\n            cv2.rectangle(rgb_vis, (x1, y1), (x2, y2), (255, 0, 0), 2)\n            cv2.putText(\n                rgb_vis,\n                f\"{cls_name} {score:.2f}\",\n                (x1, max(0, y1 - 5)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (255, 0, 0),\n                1,\n            )\n\n        plt.figure(figsize=(8, 8))\n        plt.title(\"Predictions\")\n        plt.imshow(rgb_vis)\n        plt.axis(\"off\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:04.050173Z","iopub.execute_input":"2025-12-06T21:20:04.050378Z","iopub.status.idle":"2025-12-06T21:20:04.070297Z","shell.execute_reply.started":"2025-12-06T21:20:04.050357Z","shell.execute_reply":"2025-12-06T21:20:04.069441Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# adaption merge (Faster R-CNN + ResNet-50 FPN + Lightweight Cross-Modal Attention (8x8))\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nCLASSES = load_classes()\nnum_classes = len(CLASSES)\nnum_classes_with_bg = num_classes + 1\nprint(\"Классы:\", CLASSES)\n\nsamples = collect_samples()\nprint(\"Всего валидных RGB+Depth+Label пар:\", len(samples))\nprint(\"Пример sample:\", samples[0])\n\nDEPTH_MIN, DEPTH_MAX = compute_depth_min_max(samples)\nprint(\"DEPTH_MIN, DEPTH_MAX:\", DEPTH_MIN, DEPTH_MAX)\n\nrandom.shuffle(samples)\nsplit = int(0.8 * len(samples))\ntrain_samples = samples[:split]\nval_samples = samples[split:]\n\ntrain_dataset = RGBDDetectionDataset(train_samples, IMG_SIZE)\nval_dataset = RGBDDetectionDataset(val_samples, IMG_SIZE)\n\nBATCH_SIZE = 4\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=(device == \"cuda\"),\n    collate_fn=collate_fn,\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=(device == \"cuda\"),\n    collate_fn=collate_fn,\n)\n\nprint(\"Train size:\", len(train_dataset), \"Val size:\", len(val_dataset))\n\nbackbone = RGBD_Backbone().to(device)\n\nanchor_generator = AnchorGenerator(\n    sizes=((32,), (64,), (128,), (256,), (512,)),\n    aspect_ratios=((0.5, 1.0, 2.0),) * 5,\n)\n\nroi_pooler = MultiScaleRoIAlign(\n    featmap_names=[\"0\", \"1\", \"2\", \"3\", \"pool\"],  # 5 levels\n    output_size=7,\n    sampling_ratio=2,\n)\n\nmodel = FasterRCNN(\n    backbone,\n    num_classes=num_classes_with_bg,\n    rpn_anchor_generator=anchor_generator,\n    box_roi_pool=roi_pooler,\n    image_mean=[0.5, 0.5, 0.5, 0.5],\n    image_std=[0.25, 0.25, 0.25, 0.25],\n).to(device)\n\nprint(\"Модель Faster R-CNN + Cross-Modal Attention (8x8) создана.\")\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.001,\n    momentum=0.9,\n    weight_decay=1e-4,\n)\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n    n_batches = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        total_loss += losses.item()\n        n_batches += 1\n\n    print(f\"Epoch {epoch}/{EPOCHS} | train_loss = {total_loss / n_batches:.4f}\")\n\ntorch.save(model.state_dict(), \"rgbd_fasterrcnn_crossmodal_8x8.pth\")\nprint(\"Модель сохранена: rgbd_fasterrcnn_crossmodal_8x8.pth\")\n\nshow_predictions(model, samples, score_thresh=0.6)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ITaAzzs-QEsV","outputId":"143572db-cd55-497d-f99e-8a46139e03d0","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T21:20:04.070975Z","iopub.execute_input":"2025-12-06T21:20:04.071231Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nКлассы: ['bottle', 'box', 'cola', 'container', 'cube', 'duck', 'pods', 'scissors', 'sphere', 'tape', 'tor']\nВсего валидных RGB+Depth+Label пар: 2424\nПример sample: {'rgb': '/kaggle/input/dataset/new_ds/1/images/00000.png', 'depth': '/kaggle/input/dataset/new_ds/1/depth/00000.png', 'label': '/kaggle/input/dataset/new_ds/1/labels/00000.txt', 'objects': [{'cls_id': 5, 'class_name': 'duck', 'x_center': 0.5128571428571428, 'y_center': 0.7398941798941798, 'width': 0.11523809523809515, 'height': 0.31153439153439166}]}\nDEPTH_MIN, DEPTH_MAX: 0.0 65535.0\nTrain size: 1939 Val size: 485\nМодель Faster R-CNN + Cross-Modal Attention (8x8) создана.\nEpoch 1/10 | train_loss = 0.3369\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}