{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":13930604,"datasetId":8877392,"databundleVersionId":14700256,"isSourceIdPinned":false}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\nradmilasegen_dataset_path = kagglehub.dataset_download('radmilasegen/dataset')\n\nprint('Data source import complete.')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrmPaFFSx3Ru","outputId":"13c8ab84-d3e1-4608-81ca-3503f02c70a0","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:21:54.486383Z","iopub.execute_input":"2025-12-06T22:21:54.486708Z","iopub.status.idle":"2025-12-06T22:21:56.297524Z","shell.execute_reply.started":"2025-12-06T22:21:54.486682Z","shell.execute_reply":"2025-12-06T22:21:56.296736Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom glob import glob\n\n# BASE_PATH = \"/root/.cache/kagglehub/datasets/radmilasegen/dataset/versions/1/new_ds\"\nBASE_PATH = \"/kaggle/input/dataset/new_ds\"\nCLASSES_PATH = os.path.join(BASE_PATH, \"merged_classes.txt\")","metadata":{"id":"thTd6TOdyDy_","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:21:56.298254Z","iopub.execute_input":"2025-12-06T22:21:56.298571Z","iopub.status.idle":"2025-12-06T22:21:56.302464Z","shell.execute_reply.started":"2025-12-06T22:21:56.298551Z","shell.execute_reply":"2025-12-06T22:21:56.301675Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:21:56.303991Z","iopub.execute_input":"2025-12-06T22:21:56.304372Z","iopub.status.idle":"2025-12-06T22:22:09.426386Z","shell.execute_reply.started":"2025-12-06T22:21:56.304349Z","shell.execute_reply":"2025-12-06T22:22:09.425849Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"IMG_SIZE = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.427078Z","iopub.execute_input":"2025-12-06T22:22:09.427461Z","iopub.status.idle":"2025-12-06T22:22:09.430884Z","shell.execute_reply.started":"2025-12-06T22:22:09.427431Z","shell.execute_reply":"2025-12-06T22:22:09.430214Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_classes(path=CLASSES_PATH):\n    with open(path, \"r\") as f:\n        return [line.strip() for line in f.readlines() if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.431644Z","iopub.execute_input":"2025-12-06T22:22:09.431924Z","iopub.status.idle":"2025-12-06T22:22:09.450465Z","shell.execute_reply.started":"2025-12-06T22:22:09.431893Z","shell.execute_reply":"2025-12-06T22:22:09.449943Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def read_label(label_path):\n    objects = []\n    with open(label_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != 5:\n                continue\n            cls_id = int(parts[0])\n            x, y, w, h = map(float, parts[1:])\n            objects.append({\n                \"cls_id\": cls_id,\n                \"class_name\": CLASSES[cls_id],\n                \"x_center\": x,\n                \"y_center\": y,\n                \"width\": w,\n                \"height\": h\n            })\n    return objects","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.451111Z","iopub.execute_input":"2025-12-06T22:22:09.451354Z","iopub.status.idle":"2025-12-06T22:22:09.466696Z","shell.execute_reply.started":"2025-12-06T22:22:09.451337Z","shell.execute_reply":"2025-12-06T22:22:09.466146Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def collect_samples(base_path=BASE_PATH):\n    samples = []\n    subfolders = sorted([f for f in os.listdir(base_path) if f.isdigit()])\n\n    for folder in subfolders:\n        img_dir = os.path.join(base_path, folder, \"images\")\n        depth_dir = os.path.join(base_path, folder, \"depth\")\n        label_dir = os.path.join(base_path, folder, \"labels\")\n\n        if not (os.path.isdir(img_dir) and os.path.isdir(depth_dir) and os.path.isdir(label_dir)):\n            continue\n\n        img_files = sorted(glob(os.path.join(img_dir, \"*.png\")))\n        \n        for img_path in img_files:\n            fname = os.path.basename(img_path)\n            depth_path = os.path.join(depth_dir, fname)\n            label_path = os.path.join(label_dir, fname.replace(\".png\", \".txt\"))\n\n            if not os.path.exists(depth_path):\n                continue\n            if not os.path.exists(label_path):\n                continue\n\n            objs = read_label(label_path)\n            if len(objs) == 0:\n                continue\n\n            samples.append({\n                \"rgb\": img_path,\n                \"depth\": depth_path,\n                \"label\": label_path,\n                \"objects\": objs\n            })\n    return samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.467204Z","iopub.execute_input":"2025-12-06T22:22:09.467393Z","iopub.status.idle":"2025-12-06T22:22:09.483217Z","shell.execute_reply.started":"2025-12-06T22:22:09.467378Z","shell.execute_reply":"2025-12-06T22:22:09.482725Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_depth_min_max(samples):\n    d_min = float(\"inf\")\n    d_max = float(\"-inf\")\n    for s in samples:\n        depth = cv2.imread(s[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        if depth.size == 0:\n            continue\n        m = depth.min()\n        M = depth.max()\n        if M <= 0:\n            continue\n        d_min = min(d_min, m)\n        d_max = max(d_max, M)\n\n    if not np.isfinite(d_min):\n        d_min = 0.0\n    if not np.isfinite(d_max) or d_max <= d_min:\n        d_max = d_min + 1.0\n    return d_min, d_max","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.483906Z","iopub.execute_input":"2025-12-06T22:22:09.484364Z","iopub.status.idle":"2025-12-06T22:22:09.502273Z","shell.execute_reply.started":"2025-12-06T22:22:09.484337Z","shell.execute_reply":"2025-12-06T22:22:09.501610Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class RGBDDetectionDataset(Dataset):\n    def __init__(self, samples, img_size=IMG_SIZE):\n        self.samples = samples\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n\n        rgb = cv2.imread(item[\"rgb\"])\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        rgb = cv2.resize(rgb, (self.img_size, self.img_size))\n        rgb = rgb.astype(np.float32) \n\n        depth = cv2.imread(item[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        depth = cv2.resize(depth, (self.img_size, self.img_size))\n\n        d = np.clip(depth, DEPTH_MIN, DEPTH_MAX)\n        d_norm = (d - DEPTH_MIN) / (DEPTH_MAX - DEPTH_MIN + 1e-6)\n        d_scaled = d_norm * 255.0 \n\n        depth_ch = d_scaled[..., None] \n\n        rgbd = np.concatenate([rgb, depth_ch], axis=2)\n        rgbd = rgbd / 255.0\n        rgbd = torch.from_numpy(rgbd).permute(2, 0, 1)\n\n        boxes = []\n        labels = []\n        for obj in item[\"objects\"]:\n            cx = obj[\"x_center\"]\n            cy = obj[\"y_center\"]\n            w  = obj[\"width\"]\n            h  = obj[\"height\"]\n\n            x1 = (cx - w/2.0) * self.img_size\n            y1 = (cy - h/2.0) * self.img_size\n            x2 = (cx + w/2.0) * self.img_size\n            y2 = (cy + h/2.0) * self.img_size\n\n            x1 = np.clip(x1, 0, self.img_size-1)\n            y1 = np.clip(y1, 0, self.img_size-1)\n            x2 = np.clip(x2, 0, self.img_size-1)\n            y2 = np.clip(y2, 0, self.img_size-1)\n\n            boxes.append([x1, y1, x2, y2])\n            labels.append(obj[\"cls_id\"] + 1)\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx])\n        }\n\n        return rgbd, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.504307Z","iopub.execute_input":"2025-12-06T22:22:09.504594Z","iopub.status.idle":"2025-12-06T22:22:09.520674Z","shell.execute_reply.started":"2025-12-06T22:22:09.504577Z","shell.execute_reply":"2025-12-06T22:22:09.519988Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def collate_fn(batch):\n    images, targets = list(zip(*batch))\n    return list(images), list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.521308Z","iopub.execute_input":"2025-12-06T22:22:09.521536Z","iopub.status.idle":"2025-12-06T22:22:09.541127Z","shell.execute_reply.started":"2025-12-06T22:22:09.521512Z","shell.execute_reply":"2025-12-06T22:22:09.540452Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def show_n_predictions(model, samples, n = 5, score_thresh=0.5):\n    model.eval()\n    chosen = random.sample(samples, min(5, len(samples)))\n\n    for idx, sample in enumerate(chosen):\n        print(f\"\\n============= SAMPLE {idx+1} =============\")\n\n        # читаем RGB/DEPTH так же, как в датасете\n        rgb = cv2.imread(sample[\"rgb\"])\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE))\n        rgb_vis = rgb.copy()\n\n        depth = cv2.imread(sample[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        depth = cv2.resize(depth, (IMG_SIZE, IMG_SIZE))\n        d = np.clip(depth, DEPTH_MIN, DEPTH_MAX)\n        d_norm = (d - DEPTH_MIN) / (DEPTH_MAX - DEPTH_MIN + 1e-6)\n        d_scaled = d_norm * 255.0\n        depth_ch = d_scaled[..., None]\n\n        rgbd = np.concatenate([rgb.astype(np.float32), depth_ch], axis=2) / 255.0\n        rgbd_t = torch.from_numpy(rgbd).permute(2, 0, 1).float().to(device)\n\n        with torch.no_grad():\n            outputs = model([rgbd_t])[0]\n\n        boxes = outputs[\"boxes\"].cpu().numpy()\n        labels = outputs[\"labels\"].cpu().numpy()\n        scores = outputs[\"scores\"].cpu().numpy()\n\n        for box, label, score in zip(boxes, labels, scores):\n            if score < score_thresh:\n                continue\n            x1, y1, x2, y2 = box.astype(int)\n            cls_id = int(label) - 1  # back to 0..C-1\n            if cls_id < 0 or cls_id >= len(CLASSES):\n                continue\n            cls_name = CLASSES[cls_id]\n\n            cv2.rectangle(rgb_vis, (x1, y1), (x2, y2), (255, 0, 0), 2)\n            cv2.putText(\n                rgb_vis,\n                f\"{cls_name} {score:.2f}\",\n                (x1, max(0, y1 - 5)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (255, 0, 0),\n                1,\n            )\n\n        plt.figure(figsize=(8, 8))\n        plt.title(\"Predictions\")\n        plt.imshow(rgb_vis)\n        plt.axis(\"off\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.541844Z","iopub.execute_input":"2025-12-06T22:22:09.542006Z","iopub.status.idle":"2025-12-06T22:22:09.560310Z","shell.execute_reply.started":"2025-12-06T22:22:09.541994Z","shell.execute_reply":"2025-12-06T22:22:09.559818Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# multimodal detector (Faster R-CNN + ResNet-50 FPN)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nCLASSES = load_classes()\nnum_classes = len(CLASSES)\nnum_classes_with_bg = num_classes + 1\nprint(\"Классы:\", CLASSES)\n\nsamples = collect_samples()\nprint(\"Всего валидных RGB+Depth+Label пар:\", len(samples))\nprint(\"Пример sample:\", samples[0])\n\nDEPTH_MIN, DEPTH_MAX = compute_depth_min_max(samples)\nprint(\"DEPTH_MIN, DEPTH_MAX:\", DEPTH_MIN, DEPTH_MAX)\n\nrandom.shuffle(samples)\nsplit = int(0.8 * len(samples))\ntrain_samples = samples[:split]\nval_samples   = samples[split:]\n\ntrain_dataset = RGBDDetectionDataset(train_samples, IMG_SIZE)\nval_dataset   = RGBDDetectionDataset(val_samples,   IMG_SIZE)\n\nBATCH_SIZE = 4\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=(device == \"cuda\"),\n    collate_fn=collate_fn\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=(device == \"cuda\"),\n    collate_fn=collate_fn\n)\n\nprint(\"Train size:\", len(train_dataset), \"Val size:\", len(val_dataset))\n\nbackbone = resnet_fpn_backbone('resnet50', weights=None, trainable_layers=5)\nold_conv = backbone.body.conv1\n\nbackbone.body.conv1 = nn.Conv2d(\n    in_channels=4,\n    out_channels=old_conv.out_channels,\n    kernel_size=old_conv.kernel_size,\n    stride=old_conv.stride,\n    padding=old_conv.padding,\n    bias=(old_conv.bias is not None),\n)\n\nmodel = FasterRCNN(\n    backbone,\n    num_classes=num_classes_with_bg,\n    # 4 канала → 4 значения mean/std\n    image_mean=[0.5, 0.5, 0.5, 0.5],\n    image_std=[0.25, 0.25, 0.25, 0.25],\n).to(device)\n\nprint(\"Модель Faster R-CNN RGBD создана.\")\n\nEPOCHS = 10\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.001,\n    momentum=0.9,\n    weight_decay=1e-4\n)\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n    n_samples = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        bs = len(images)\n        total_loss += losses.item() * bs\n        n_samples += bs\n\n    print(f\"Epoch {epoch}/{EPOCHS} | train_loss = {total_loss / n_samples:.4f}\")\n\ntorch.save(model.state_dict(), \"rgbd_fasterrcnn_resnet50.pth\")\nprint(\"Модель сохранена: rgbd_fasterrcnn_resnet50.pth\")\n\nshow_n_predictions(model, samples, n = 5, score_thresh=0.6)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"E-eA282JyFj9","outputId":"a2a98bc3-21d5-4208-d8eb-9b5b4a3c1f13","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T22:22:09.560999Z","iopub.execute_input":"2025-12-06T22:22:09.561170Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nКлассы: ['bottle', 'box', 'cola', 'container', 'cube', 'duck', 'pods', 'scissors', 'sphere', 'tape', 'tor']\nВсего валидных RGB+Depth+Label пар: 2424\nПример sample: {'rgb': '/kaggle/input/dataset/new_ds/1/images/00000.png', 'depth': '/kaggle/input/dataset/new_ds/1/depth/00000.png', 'label': '/kaggle/input/dataset/new_ds/1/labels/00000.txt', 'objects': [{'cls_id': 5, 'class_name': 'duck', 'x_center': 0.5128571428571428, 'y_center': 0.7398941798941798, 'width': 0.11523809523809515, 'height': 0.31153439153439166}]}\nDEPTH_MIN, DEPTH_MAX: 0.0 65535.0\nTrain size: 1939 Val size: 485\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Модель Faster R-CNN RGBD создана.\nEpoch 1/10 | train_loss = 0.4456\n","output_type":"stream"}],"execution_count":null}]}