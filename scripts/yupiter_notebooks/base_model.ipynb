{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14085872,"sourceType":"datasetVersion","datasetId":8968270,"isSourceIdPinned":false}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\nradmilasegen_dataset_path = kagglehub.dataset_download('radmilasegen/dataset-fin')\n\nprint('Data source import complete.')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrmPaFFSx3Ru","outputId":"13c8ab84-d3e1-4608-81ca-3503f02c70a0","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:44.818789Z","iopub.execute_input":"2025-12-10T13:45:44.819026Z","iopub.status.idle":"2025-12-10T13:45:45.285248Z","shell.execute_reply.started":"2025-12-10T13:45:44.819000Z","shell.execute_reply":"2025-12-10T13:45:45.284419Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nBASE_PATH = \"/kaggle/input/dataset-fin/new_ds\"\nCLASSES_PATH = os.path.join(BASE_PATH, \"merged_classes.txt\")","metadata":{"id":"thTd6TOdyDy_","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:45.287221Z","iopub.execute_input":"2025-12-10T13:45:45.287469Z","iopub.status.idle":"2025-12-10T13:45:45.291027Z","shell.execute_reply.started":"2025-12-10T13:45:45.287452Z","shell.execute_reply":"2025-12-10T13:45:45.290271Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.resnet import resnet50, ResNet50_Weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:45.292428Z","iopub.execute_input":"2025-12-10T13:45:45.292689Z","iopub.status.idle":"2025-12-10T13:45:58.020567Z","shell.execute_reply.started":"2025-12-10T13:45:45.292672Z","shell.execute_reply":"2025-12-10T13:45:58.019799Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"IMG_SIZE = 256\nEPOCHS = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.021391Z","iopub.execute_input":"2025-12-10T13:45:58.021709Z","iopub.status.idle":"2025-12-10T13:45:58.025113Z","shell.execute_reply.started":"2025-12-10T13:45:58.021690Z","shell.execute_reply":"2025-12-10T13:45:58.024348Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_classes(path=CLASSES_PATH):\n    with open(path, \"r\") as f:\n        return [line.strip() for line in f.readlines() if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.025953Z","iopub.execute_input":"2025-12-10T13:45:58.026193Z","iopub.status.idle":"2025-12-10T13:45:58.048355Z","shell.execute_reply.started":"2025-12-10T13:45:58.026175Z","shell.execute_reply":"2025-12-10T13:45:58.047821Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def read_label(label_path):\n    objects = []\n    with open(label_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != 5:\n                continue\n            cls_id = int(parts[0])\n            x, y, w, h = map(float, parts[1:])\n            objects.append({\n                \"cls_id\": cls_id,\n                \"class_name\": CLASSES[cls_id],\n                \"x_center\": x,\n                \"y_center\": y,\n                \"width\": w,\n                \"height\": h\n            })\n    return objects","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.049088Z","iopub.execute_input":"2025-12-10T13:45:58.049274Z","iopub.status.idle":"2025-12-10T13:45:58.064822Z","shell.execute_reply.started":"2025-12-10T13:45:58.049259Z","shell.execute_reply":"2025-12-10T13:45:58.064175Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def collect_samples(base_path=BASE_PATH):\n    samples = []\n    subfolders = sorted([f for f in os.listdir(base_path) if f.isdigit()])\n\n    for folder in subfolders:\n        img_dir = os.path.join(base_path, folder, \"images\")\n        depth_dir = os.path.join(base_path, folder, \"depth\")\n        label_dir = os.path.join(base_path, folder, \"labels\")\n\n        if not (os.path.isdir(img_dir) and os.path.isdir(depth_dir) and os.path.isdir(label_dir)):\n            continue\n\n        img_files = sorted(glob(os.path.join(img_dir, \"*.png\")))\n        \n        for img_path in img_files:\n            fname = os.path.basename(img_path)\n            depth_path = os.path.join(depth_dir, fname)\n            label_path = os.path.join(label_dir, fname.replace(\".png\", \".txt\"))\n\n            if not os.path.exists(depth_path):\n                continue\n            if not os.path.exists(label_path):\n                continue\n\n            objs = read_label(label_path)\n            if len(objs) == 0:\n                continue\n\n            samples.append({\n                \"rgb\": img_path,\n                \"depth\": depth_path,\n                \"label\": label_path,\n                \"objects\": objs\n            })\n    return samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.066770Z","iopub.execute_input":"2025-12-10T13:45:58.067093Z","iopub.status.idle":"2025-12-10T13:45:58.081464Z","shell.execute_reply.started":"2025-12-10T13:45:58.067074Z","shell.execute_reply":"2025-12-10T13:45:58.080813Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_depth_min_max(samples):\n    d_min = float(\"inf\")\n    d_max = float(\"-inf\")\n    for s in samples:\n        depth = cv2.imread(s[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        if depth.size == 0:\n            continue\n        m = depth.min()\n        M = depth.max()\n        if M <= 0:\n            continue\n        d_min = min(d_min, m)\n        d_max = max(d_max, M)\n\n    if not np.isfinite(d_min):\n        d_min = 0.0\n    if not np.isfinite(d_max) or d_max <= d_min:\n        d_max = d_min + 1.0\n    return d_min, d_max","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.082176Z","iopub.execute_input":"2025-12-10T13:45:58.082379Z","iopub.status.idle":"2025-12-10T13:45:58.103565Z","shell.execute_reply.started":"2025-12-10T13:45:58.082354Z","shell.execute_reply":"2025-12-10T13:45:58.103063Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class RGBDDetectionDataset(Dataset):\n    def __init__(self, samples, img_size=IMG_SIZE):\n        self.samples = samples\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n\n        rgb = cv2.imread(item[\"rgb\"])\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        rgb = cv2.resize(rgb, (self.img_size, self.img_size))\n        rgb = rgb.astype(np.float32) \n\n        depth = cv2.imread(item[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        depth = cv2.resize(depth, (self.img_size, self.img_size))\n\n        d = np.clip(depth, DEPTH_MIN, DEPTH_MAX)\n        d_norm = (d - DEPTH_MIN) / (DEPTH_MAX - DEPTH_MIN + 1e-6)\n        d_scaled = d_norm * 255.0 \n\n        depth_ch = d_scaled[..., None] \n\n        rgbd = np.concatenate([rgb, depth_ch], axis=2)\n        rgbd = rgbd / 255.0\n        rgbd = torch.from_numpy(rgbd).permute(2, 0, 1)\n\n        boxes = []\n        labels = []\n        for obj in item[\"objects\"]:\n            cx = obj[\"x_center\"]\n            cy = obj[\"y_center\"]\n            w  = obj[\"width\"]\n            h  = obj[\"height\"]\n\n            x1 = (cx - w/2.0) * self.img_size\n            y1 = (cy - h/2.0) * self.img_size\n            x2 = (cx + w/2.0) * self.img_size\n            y2 = (cy + h/2.0) * self.img_size\n\n            x1 = np.clip(x1, 0, self.img_size-1)\n            y1 = np.clip(y1, 0, self.img_size-1)\n            x2 = np.clip(x2, 0, self.img_size-1)\n            y2 = np.clip(y2, 0, self.img_size-1)\n\n            boxes.append([x1, y1, x2, y2])\n            labels.append(obj[\"cls_id\"] + 1)\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx])\n        }\n\n        return rgbd, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.104244Z","iopub.execute_input":"2025-12-10T13:45:58.104474Z","iopub.status.idle":"2025-12-10T13:45:58.120576Z","shell.execute_reply.started":"2025-12-10T13:45:58.104453Z","shell.execute_reply":"2025-12-10T13:45:58.119913Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def collate_fn(batch):\n    images, targets = list(zip(*batch))\n    return list(images), list(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.121819Z","iopub.execute_input":"2025-12-10T13:45:58.122094Z","iopub.status.idle":"2025-12-10T13:45:58.140368Z","shell.execute_reply.started":"2025-12-10T13:45:58.122074Z","shell.execute_reply":"2025-12-10T13:45:58.139888Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def show_n_predictions(model, samples, n = 5, score_thresh=0.5):\n    model.eval()\n    chosen = random.sample(samples, min(5, len(samples)))\n\n    for idx, sample in enumerate(chosen):\n        print(f\"\\ntest sample {idx+1}\")\n\n        rgb = cv2.imread(sample[\"rgb\"])\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE))\n        rgb_vis = rgb.copy()\n\n        depth = cv2.imread(sample[\"depth\"], cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        depth = cv2.resize(depth, (IMG_SIZE, IMG_SIZE))\n        d = np.clip(depth, DEPTH_MIN, DEPTH_MAX)\n        d_norm = (d - DEPTH_MIN) / (DEPTH_MAX - DEPTH_MIN + 1e-6)\n        d_scaled = d_norm * 255.0\n        depth_ch = d_scaled[..., None]\n\n        rgbd = np.concatenate([rgb.astype(np.float32), depth_ch], axis=2) / 255.0\n        rgbd_t = torch.from_numpy(rgbd).permute(2, 0, 1).float().to(device)\n\n        with torch.no_grad():\n            outputs = model([rgbd_t])[0]\n\n        boxes = outputs[\"boxes\"].cpu().numpy()\n        labels = outputs[\"labels\"].cpu().numpy()\n        scores = outputs[\"scores\"].cpu().numpy()\n\n        for box, label, score in zip(boxes, labels, scores):\n            if score < score_thresh:\n                continue\n            x1, y1, x2, y2 = box.astype(int)\n            cls_id = int(label) - 1 \n            if cls_id < 0 or cls_id >= len(CLASSES):\n                continue\n            cls_name = CLASSES[cls_id]\n\n            cv2.rectangle(rgb_vis, (x1, y1), (x2, y2), (255, 0, 0), 2)\n            cv2.putText(\n                rgb_vis,\n                f\"{cls_name} {score:.2f}\",\n                (x1, max(0, y1 - 5)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (255, 0, 0),\n                1,\n            )\n\n        plt.figure(figsize=(8, 8))\n        plt.title(\"Predictions\")\n        plt.imshow(rgb_vis)\n        plt.axis(\"off\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.141108Z","iopub.execute_input":"2025-12-10T13:45:58.141319Z","iopub.status.idle":"2025-12-10T13:45:58.156742Z","shell.execute_reply.started":"2025-12-10T13:45:58.141300Z","shell.execute_reply":"2025-12-10T13:45:58.156142Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def box_iou(box1, box2):\n    x1 = max(float(box1[0]), float(box2[0]))\n    y1 = max(float(box1[1]), float(box2[1]))\n    x2 = min(float(box1[2]), float(box2[2]))\n    y2 = min(float(box1[3]), float(box2[3]))\n\n    inter_w = max(0.0, x2 - x1)\n    inter_h = max(0.0, y2 - y1)\n    inter = inter_w * inter_h\n\n    if inter <= 0.0:\n        return 0.0\n\n    area1 = max(0.0, float(box1[2]) - float(box1[0])) * max(0.0, float(box1[3]) - float(box1[1]))\n    area2 = max(0.0, float(box2[2]) - float(box2[0])) * max(0.0, float(box2[3]) - float(box2[1]))\n    union = area1 + area2 - inter\n    if union <= 0.0:\n        return 0.0\n\n    return inter / union","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.157469Z","iopub.execute_input":"2025-12-10T13:45:58.157674Z","iopub.status.idle":"2025-12-10T13:45:58.179354Z","shell.execute_reply.started":"2025-12-10T13:45:58.157659Z","shell.execute_reply":"2025-12-10T13:45:58.178849Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate_detection(model, data_loader, class_names, device=None,\n                    iou_thresh=0.5, score_thresh=0.5):\n    if device is None:\n        device = next(model.parameters()).device\n\n    model.eval()\n\n    num_classes = len(class_names)\n\n    total_gt = 0 \n    total_pred = 0\n    total_tp = 0 \n\n    iou_sum = 0.0\n    iou_cnt = 0\n\n    per_class_gt = [0] * num_classes\n    per_class_pred = [0] * num_classes\n    per_class_tp = [0] * num_classes\n    per_class_iou_sum = [0.0] * num_classes\n    per_class_iou_cnt = [0] * num_classes\n\n    for images, targets in data_loader:\n        images = [img.to(device) for img in images]\n\n        outputs = model(images)\n\n        for out, tgt in zip(outputs, targets):\n            gt_boxes = tgt[\"boxes\"].cpu().numpy()\n            gt_labels = tgt[\"labels\"].cpu().numpy() \n\n            total_gt += len(gt_boxes)\n            for lbl in gt_labels:\n                if 1 <= lbl <= num_classes:\n                    per_class_gt[lbl - 1] += 1\n\n            if len(gt_boxes) == 0:\n                pred_scores = out[\"scores\"].cpu().numpy()\n                pred_labels = out[\"labels\"].cpu().numpy()\n\n                keep = pred_scores >= score_thresh\n                pred_labels = pred_labels[keep]\n\n                num_pred = len(pred_labels)\n                total_pred += num_pred\n                for lbl in pred_labels:\n                    if 1 <= lbl <= num_classes:\n                        per_class_pred[lbl - 1] += 1\n                continue\n\n            pred_boxes = out[\"boxes\"].cpu().numpy()\n            pred_labels = out[\"labels\"].cpu().numpy()\n            pred_scores = out[\"scores\"].cpu().numpy()\n\n            keep = pred_scores >= score_thresh\n            pred_boxes = pred_boxes[keep]\n            pred_labels = pred_labels[keep]\n            pred_scores = pred_scores[keep]\n\n            total_pred += len(pred_boxes)\n            for lbl in pred_labels:\n                if 1 <= lbl <= num_classes:\n                    per_class_pred[lbl - 1] += 1\n\n            matched_gt = np.zeros(len(gt_boxes), dtype=bool)\n\n            order = np.argsort(-pred_scores)\n            for p_idx in order:\n                p_box = pred_boxes[p_idx]\n                p_lbl = pred_labels[p_idx]\n                if not (1 <= p_lbl <= num_classes):\n                    continue\n\n                best_iou = 0.0\n                best_gt_idx = -1\n\n                for g_idx, (g_box, g_lbl) in enumerate(zip(gt_boxes, gt_labels)):\n                    if matched_gt[g_idx]:\n                        continue\n                    if g_lbl != p_lbl:\n                        continue\n\n                    iou = box_iou(p_box, g_box)\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_gt_idx = g_idx\n\n                if best_gt_idx >= 0 and best_iou >= iou_thresh:\n                    matched_gt[best_gt_idx] = True\n                    total_tp += 1\n\n                    iou_sum += best_iou\n                    iou_cnt += 1\n\n                    c = p_lbl - 1\n                    per_class_tp[c] += 1\n                    per_class_iou_sum[c] += best_iou\n                    per_class_iou_cnt[c] += 1\n                else:\n                    pass\n\n    def safe_div(a, b):\n        return a / b if b > 0 else 0.0\n\n    overall_precision = safe_div(total_tp, total_pred)\n    overall_recall = safe_div(total_tp, total_gt)\n    overall_f1 = safe_div(2 * overall_precision * overall_recall,\n                        overall_precision + overall_recall)\n    overall_iou = safe_div(iou_sum, iou_cnt)\n\n    print(\"Detection metrics: IoU >= {:.2f}, score >= {:.2f}\".format(\n        iou_thresh, score_thresh))\n    print(f\"  GT: {total_gt}, Pred: {total_pred}, TP: {total_tp}\")\n    print(f\"Precision: {overall_precision:.4f}\")\n    print(f\"Recall:    {overall_recall:.4f}\")\n    print(f\"F1-score:  {overall_f1:.4f}\")\n    print(f\"Avg IoU by TP: {overall_iou:.4f}\")\n\n    print(\"\\nMetrics by classes:\")\n    for idx, name in enumerate(class_names):\n        tp = per_class_tp[idx]\n        gt = per_class_gt[idx]\n        pred = per_class_pred[idx]\n        prec = safe_div(tp, pred)\n        rec = safe_div(tp, gt)\n        f1 = safe_div(2 * prec * rec, prec + rec)\n        avg_iou = safe_div(per_class_iou_sum[idx], per_class_iou_cnt[idx])\n\n        print(f\"[{idx+1}] {name}:\")\n        print(f\"  GT: {gt}, Pred: {pred}, TP: {tp}\")\n        print(f\"  Precision: {prec:.4f}\")\n        print(f\"  Recall:    {rec:.4f}\")\n        print(f\"  F1-score:  {f1:.4f}\")\n        print(f\"  Avg IoU:   {avg_iou:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.180050Z","iopub.execute_input":"2025-12-10T13:45:58.180231Z","iopub.status.idle":"2025-12-10T13:45:58.200844Z","shell.execute_reply.started":"2025-12-10T13:45:58.180217Z","shell.execute_reply":"2025-12-10T13:45:58.200350Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# multimodal detector (Faster R-CNN + ResNet-50 FPN)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nCLASSES = load_classes()\nnum_classes = len(CLASSES)\nnum_classes_with_bg = num_classes + 1\nprint(\"Классы:\", CLASSES)\n\nsamples = collect_samples()\nprint(\"Всего валидных RGB+Depth+Label пар:\", len(samples))\nprint(\"Пример sample:\", samples[0])\n\nDEPTH_MIN, DEPTH_MAX = compute_depth_min_max(samples)\nprint(\"DEPTH_MIN, DEPTH_MAX:\", DEPTH_MIN, DEPTH_MAX)\n\nrandom.shuffle(samples)\nsplit = int(0.75 * len(samples))\ntrain_samples = samples[:split]\ntest_samples   = samples[split:]\n\ntrain_dataset = RGBDDetectionDataset(train_samples, IMG_SIZE)\ntest_dataset   = RGBDDetectionDataset(test_samples,   IMG_SIZE)\n\nBATCH_SIZE = 4\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=(device == \"cuda\"),\n    collate_fn=collate_fn\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=(device == \"cuda\"),\n    collate_fn=collate_fn\n)\n\nprint(\"Train size:\", len(train_dataset), \"Test size:\", len(test_dataset))\n\nbackbone = resnet_fpn_backbone('resnet50', weights=ResNet50_Weights.IMAGENET1K_V1, trainable_layers=5)\nold_conv = backbone.body.conv1\n\nbackbone.body.conv1 = nn.Conv2d(\n    in_channels=4,\n    out_channels=old_conv.out_channels,\n    kernel_size=old_conv.kernel_size,\n    stride=old_conv.stride,\n    padding=old_conv.padding,\n    bias=(old_conv.bias is not None),\n)\n\nmodel = FasterRCNN(\n    backbone,\n    num_classes=num_classes_with_bg,\n    image_mean=[0.5, 0.5, 0.5, 0.5],\n    image_std=[0.25, 0.25, 0.25, 0.25],\n).to(device)\n\nprint(\"Model created.\")\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.001,\n    momentum=0.9,\n    weight_decay=1e-4\n)\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n    n_samples = 0\n\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        bs = len(images)\n        total_loss += losses.item() * bs\n        n_samples += bs\n\n    print(f\"Epoch {epoch}/{EPOCHS} | train_loss = {total_loss / n_samples:.4f}\")\n\ntorch.save(model.state_dict(), \"rgbd_fasterrcnn_resnet50.pth\")\nprint(\"Модель сохранена: rgbd_fasterrcnn_resnet50.pth\")\n\nshow_n_predictions(model, samples, n = 5, score_thresh=0.1)\n\nevaluate_detection(model, test_loader, CLASSES, device=device, iou_thresh=0.1, score_thresh=0.1)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"E-eA282JyFj9","outputId":"a2a98bc3-21d5-4208-d8eb-9b5b4a3c1f13","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T13:45:58.201497Z","iopub.execute_input":"2025-12-10T13:45:58.201682Z","iopub.status.idle":"2025-12-10T13:52:30.893618Z","shell.execute_reply.started":"2025-12-10T13:45:58.201668Z","shell.execute_reply":"2025-12-10T13:52:30.892570Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nКлассы: ['bottle', 'box', 'cola', 'container', 'cube', 'duck', 'pods', 'scissors', 'sphere', 'tape', 'tor']\nВсего валидных RGB+Depth+Label пар: 3118\nПример sample: {'rgb': '/kaggle/input/dataset-fin/new_ds/1/images/00000.png', 'depth': '/kaggle/input/dataset-fin/new_ds/1/depth/00000.png', 'label': '/kaggle/input/dataset-fin/new_ds/1/labels/00000.txt', 'objects': [{'cls_id': 6, 'class_name': 'pods', 'x_center': 0.5128571428571428, 'y_center': 0.7398941798941798, 'width': 0.11523809523809515, 'height': 0.31153439153439166}]}\nDEPTH_MIN, DEPTH_MAX: 0.0 65535.0\nTrain size: 2338 Test size: 780\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 196MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model created.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4025328743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14}]}