# Исследование мультимодального обнаружения объектов в условиях сильных перекрытий

## Участники 

- Куранова Полина - 466426
- Сегень Радмила (https://github.com/Radmilase) - 472460
- Дерина Вероника (https://github.com/VerDddD) - 465747
- Талканов Амир (https://github.com/BegomAmir) - 412420

## Описание исследования
В ходе выполнения курсовой работы по дисциплине "Методы машинного обучения в робототехнике" было выполнено исследование по теме мультимодального обнаружения объектов в условиях сильных перекрытий.

### Цель исследования
Провести сравнительный анализ 4 моделей для мультимодального обнаружения объектов в условиях сильных перекрытий.

Задача мультимодальной детекции объектов подразумевает под собой детекцию объектов на основе информации имеющей разную природу. В нашем случе используется rgb изображение и карта глубины с камеры, а так же в 2 случаях оцененная карта глубины полученная с помощью внешней модели [MiDaS](https://github.com/isl-org/MiDaS).

### Гипотеза
Модель с механизмом внимания и дополнительным каналом синтезированной глубины должна демонстрировать наилучшее качество детекции по сравнению с альтернативной моделью с механизмом gating и более простыми версиями.

### Задачи исследования
- Снять датасет (определить детектируемые объекты, объем датасета и снимаемые сцены).
- Разметить датасет (использовалась платформа Label Studio).
- Реализовать модели.
- Обучить модели, рассчитать метрики качества обучения модели.
- Реализовать приложение для тестирования работы в режиме реального времени.
- Протестировать приложение для каждой модели, рассчитать среднее время обработки кадра для каждой модели.

### Описание репозитория

В данном репозитории представлен исходный код используемый для проведения исследования.

В папке notebooks представлены скрипты ипользуемые для обучения моделей, а в папке src представлен исходный код необходимый для работы приложения. Главный файл приложения main_test.py.

### Описание датасета
Данный датасет был собран и размечен вручную. Съёмка данных выполнялась с использованием RGB-D камеры Intel RealSense, с помощью которой получили синхронизированные цветные изображения и карты глубины.
Объекты снимались в условиях частичных перекрытий объектов с наличием отражающих поверхностей. Объекты располагались в произвольных положениях и ориентациях. Во большинстве сценах более двух объектов одновременно, в том числе частично перекрывающихся между собой либо другими предметами.

#### Объекты и классы
Были взяты как простые объекты, так и сложный: шары различных цветов; кольцевые объекты различного размера и цвета; бутылка железная; бутылка пластиквая; деревянные кубики; игрушечная уточка; ножницы; скотч.
Каждый объект отнесён к одному из следующих классов: bottle, cube, duck, scissors, sphere, tape, tor.

#### Разметка данных
Разметка датасета выполнялась вручную при помощи Label Studio. Каждый целевой объект выделялся в виде bounding box и ему присваивалась метка класса. Формат разметки совместим с Faster R-CNN и YOLO.

### Инструкция по установке датсета
Датасет на котором обучались модели находится по ссылке: https://www.kaggle.com/datasets/radmilasegen/dataset-fin
Для его установки используйте следующие команды:

```python
import kagglehub

# Download latest version
path = kagglehub.dataset_download("radmilasegen/dataset-fin")

print("Path to dataset files:", path)
```
В файле [merged_classes.txt](merged_classes.txt) представлен список детектируемых объектов. Этот же файл можно найти в датасете.


### Описание моделей
В проекте представлен путь модификаций четырёхканальной rgb-d модели с простейшим слиянием, основанной на статье [Feature Calibrating and Fusing Network for RGB-D Salient Object Detection](https://ieeexplore.ieee.org/abstract/document/10185946)
, в дальнейшем базовая модель.

#### Модель 1: Базовая модель (rgbd_base_model)
В базовой модели происходит прямое объединение признаков rgb и карты реальной глубины, что означает их равный вклад, при этом никак не учитывается качество данных. Данные реальной глубины могут содержать шумы и
артефакты, а объекты на изображениях rgb перекрыты.

<img width="2497" height="882" alt="image" src="https://github.com/user-attachments/assets/d3df0f48-c2b0-4648-9335-5fa3ebb43f32" />

#### Модель 2: 4-х канальная модель с механизмом гейтирования (gating_model)
На первом этапе модификации базовой модели вместо слияния был добавлен механизм gating, принцип которого заключается в адаптивном взвешивании признаков rgb и реальной глубины. В модуле gating используется обучаемое значение - гейт, который, в зависимости от сцены, может усиливать или ослаблять вклад одной из двух модальностей, rgb или глубины.

<img width="2530" height="875" alt="image" src="https://github.com/user-attachments/assets/707eb262-4bff-4a83-9216-42a6b6bda84f" />

#### Модель 3: 5-ти канальная модель с механизмом гейтирования (rgbdd_gating_model)
Второй этап модификации добавляет в архитектуру пятый канал данных - оцененную глубину, синтезированную с помощью нейросети Midas. Смысл добавления этих данных заключается в том, что реальная глубина может быть сильно зашумлена или же пропускать объекты из-за их дальнего расположения от камеры, или при наличии в сценах светоотражающих или светопропускающих поверхностей. В таких случаях оцененная глубина сможет заменить реальную, а также очертить границы объектов. 

<img width="2890" height="885" alt="image" src="https://github.com/user-attachments/assets/3bf6dbe4-0b99-4ddb-9019-449e512796bd" />

#### Модель 4: 5-ти канальная модель с механизмом внимания (rgbdd_attention_model)
В третьем этапе модификации был реализован механизм attention, который, в отличии от gating, формирует зависимость между весами rgb, реальной глубины и оцененной глубины. Что позволит модели выбирать наиболее информативный источник данных, что поможет детекции в сценах с сильными перекрытиями(окклюзиями) объектов.

<img width="2882" height="870" alt="image" src="https://github.com/user-attachments/assets/3ae61d62-4641-4bb6-86c5-1c4b5f90a517" />

В процессе экспериментов обучение моделей производилось на 30 эпохах.

### Описание используемых метрик

Ниже приведены суммарные показатели качества обнаружения для четырех моделей:
*базовой*, модели с *gating*, модели с *gating + оценкой глубины* и модели с *Cross-Attention +
Depth*. Показаны стандартные метрики: Precision (точность), Recall (полнота), F1-score и средний
IoU (пересечение по объединению) для обнаруженных объектов:

| Модель                         | Precision | Recall | F1-score | Avg IoU |
|--------------------------------|-----------|--------|----------|---------|
| Base model (IoU ≥ 0.10)         | **0.5262**    | **0.9820** | **0.6853**   | **0.8235**  |
| Gating model                   | 0.7487    | 0.7544 | 0.7515   | 0.8718  |
| Gating + Depth Assessment      | 0.7865    | 0.7450 | **0.7652**   | 0.8714  |
| Cross-Attention + Depth        | 0.7494    | 0.7638 | 0.7566   | 0.8697  |

**Base model.**
Базовая модель обеспечивает почти максимальную полноту обнаружения ```(Recall ≈ 98%)```, но страдает от крайне низкой точности и большого числа ложных срабатываний. Низкий ```IoU``` указывает на грубую локализацию объектов. Она подходит лишь как первичный детектор, если дальнейшая фильтрация ошибок предусмотрена.

**Gating model.**
Gating-модель достигает сбалансированного соотношения точности и полноты ```(около 75%)```, существенно снижая количество ложных тревог по сравнению с базовой моделью. Это даёт заметный рост ```F1-меры``` при относительно простой архитектуре. Модель подходит для универсальных сценариев без использования данных глубины.

**Gating + Depth Assessment.**
Модель с оценкой глубины демонстрирует наивысшие значения ```Precision, F1 и IoU```, обеспечивая наиболее точную детекцию объектов. При этом Recall остаётся высоким и лишь незначительно уступает более простым решениям. В большинстве практических задач она является оптимальным выбором.

**Cross-Attention + Depth.**
Использование ```cross-attention``` позволяет улучшить обнаружение отдельных сложных объектов и немного повысить полноту. Однако это достигается ценой снижения точности и увеличения вычислительной сложности. Модель оправдана в сценариях, где важно максимальное обнаружение трудных случаев.


### Описание приложения
Для проверки работы моделей в режиме реального времени было раработано приложение (инструкция по его запуску представлена в разделе "Устанока и развертывание").

Данное приложение использует библиотеку pyrealsense2 и считывает изображение (rgb + глубина) с камеры RealSense и далее передает на одну из моделей выбранных в конфигурационном файле. Если работа производится для моделей 3 и 4, то предварительно с помощью модели MiDaS по rgb изображению формируется 5 канал информации - оцененная глубина. Она тоже передается в модель.

Для оценки скорости обработки моделей расчитывается время обработки 1 кадра.

В конце приложение визуализирует полученнное rgb избражение с детектируемыми на нем объектами. Так же визуализируются карты глубины (одна для моделей 1 и 2, две для моделей 3 и 4).

## Демонстрация работы

*TODO*

ссылка на видео

gif анимация

## Устанока и развертывание

Обучение моделей производилось с использованием вычислительных мощностей сред Google Colab и Kaggle. Используемые скрипты представлены в папке notebooks.

Приложение для тестирования работы моделей на данных с видеокамеры RealSense разрабатывалось с использованием окружения Anaconda. Файл с используемым окружением: environment.yaml.

Для развертки окружения выполните следующие команды:
- Убедитесь, что у вас установлен Anaconda или Miniconda.
- Создайте окружение на основе файла environment.yml: 
```bash
conda env create -f environment.yml
```
- Активируйте окружение:
```bash
conda activate anaconda_env  # или имя, указанное в поле name в environment.yml
```
- Если зависимости обновлялись, синхронизируйте окружение:
```bash
conda env update --file environment.yml --prune
```

## Запуск и использование

### Обучение моделей
Для повторения процесса обучения моделей необходимо запустить скрипты представленые в папке notebooks. Их можно скачать из репозитория и вреучную выгрузить в Google Colab или Kaggle. Рекомендуется использовать GPU для более быстрого обучения.

Так же скрипты можно запустить локально, но для этого требуются минимум 7.28 Гб свободной памяти для датасета и достаточные вычислительные мощности.

### Конфигурация приложения
Для конфигурации работы приложения необходимо заполнить файл config/config.py.
Необходимо указать значение следующим переменным:
- CLASSES_PATH - путь к файлу с описанием классов,
- MODEL - название используемой модели,
- MODEL_PATH - путь к весам обученной модели,
- DEPTH_MIN, DEPTH_MAX, IMG_SIZE - параметры обучения.

### Запуск приложения для тестирования работы моделей на данных с видеокамеры RealSense
- загрузка репозитория:
```bash
git@github.com:KuranovaPolina/multimodal_object_detection.git
```
- Активация окружения: 
```bash
conda activate anaconda_env
```
- Запуск: 
```bash
python main_test.py
```

## Описание полученных результатов

*TODO*

------- Сравнение метрик в таблице

