# Исследование мультимодального обнаружения объектов в условиях сильных перекрытий

## Участники 

- Куранова Полина - 466426
- Сегень Радмила (https://github.com/Radmilase) - 472460
- Дерина Вероника (https://github.com/VerDddD) - 465747
- Талканов Амир (https://github.com/BegomAmir) - 412420

## Описание исследования
В ходе выполнения курсовой работы по дисциплине "Методы машинного обучения в робототехнике" было выполнено исследование по теме мультимодального обнаружения объектов в условиях сильных перекрытий.

### Цель исследования
Провести сравнительный анализ 4 моделей для мультимодального обнаружения объектов в условиях сильных перекрытий.

Задача мультимодальной детекции объектов подразумевает под собой детекцию объектов на основе информации имеющей разную природу. В нашем случе используется rgb изображение и карта глубины с камеры, а так же в 2 случаях оцененная карта глубины полученная с помощью внешней модели [MiDaS](https://github.com/isl-org/MiDaS).

### Гипотеза
Модель с механизмом внимания и дополнительным каналом синтезированной глубины должна демонстрировать наилучшее качество детекции по сравнению с альтернативной моделью с механизмом gating и более простыми версиями.

### Задачи исследования
- Снять датасет (определить детектируемые объекты, объем датасета и снимаемые сцены).
- Разметить датасет (использовалась платформа Label Studio).
- Реализовать модели.
- Обучить модели, рассчитать метрики качества обучения модели.
- Реализовать приложение для тестирования работы в режиме реального времени.
- Протестировать приложение для каждой модели, рассчитать среднее время обработки кадра для каждой модели.

### Описание репозитория

В данном репозитории представлен исходный код используемый для проведения исследования.

В папке notebooks представлены скрипты ипользуемые для обучения моделей, а в папке src представлен исходный код необходимый для работы приложения. Главный файл приложения [main_test.py](https://github.com/KuranovaPolina/multimodal_object_detection/blob/main/main_test.py).

### Описание датасета
Данный датасет был собран и размечен вручную. Он включает 3192 пары данных, каждая из которых состоит из RGB-изображения и соответствующей карты глубины, полученных синхронно. Съёмка данных выполнялась с использованием RGB-D камеры Intel RealSense.
Объекты снимались в условиях частичных перекрытий объектов с наличием отражающих поверхностей. Объекты располагались в произвольных положениях и ориентациях. Во большинстве сценах более двух объектов одновременно, в том числе частично перекрывающихся между собой либо другими предметами.

#### Объекты и классы
Были взяты как простые объекты, так и сложный: шары различных цветов; кольцевые объекты различного размера и цвета; бутылка железная; бутылка пластиквая; деревянные кубики; игрушечная уточка; ножницы; скотч.
Каждый объект отнесён к одному из следующих классов: bottle, cube, duck, scissors, sphere, tape, tor.

#### Разметка данных
Разметка датасета выполнялась вручную при помощи [Label Studio](https://labelstud.io/). Каждый целевой объект выделялся в виде bounding box и ему присваивалась метка класса. Формат разметки совместим с Faster R-CNN и YOLO.

ПАРА КАРТИНОК

### Инструкция по установке датсета
Датасет на котором обучались модели находится по [ссылке](https://www.kaggle.com/datasets/radmilasegen/dataset-fin).
Для его установки используйте следующие команды:

```python
import kagglehub

# Download latest version
path = kagglehub.dataset_download("radmilasegen/dataset-fin")

print("Path to dataset files:", path)
```
В файле [merged_classes.txt](merged_classes.txt) представлен список детектируемых объектов. Этот же файл можно найти в датасете.


### Описание моделей
В проекте представлен путь модификаций четырёхканальной RGB-D модели с простейшим слиянием, основанной на статье [Multimodal Object Detection using Depth and Image Data for Manufacturing Parts](https://www.researchgate.net/publication/385823214_Multimodal_Object_Detection_using_Depth_and_Image_Data_for_Manufacturing_Parts)
, в дальнейшем базовая модель.

#### Модель 1: Базовая модель (RGB-D Faster R-CNN с ранним слиянием входных данных и ResNet-50 + FPN backbone)
Модель основана на [Faster R-CNN](https://arxiv.org/abs/1506.01497) с backbone от ResNet50, а затем расширяется с помощью FPN (Feature Pyramid Network).
На вход подается цветное изображение с 3-мя каналами RGB и 1 канал глубины Depth, преварительно нормализованный. Далее происходит ранее слияние (early fusion), то есть конкатенация каналов. Получившийся 4-х канальный тензор подается на ResNet50. Для обработки RGB-D в первой слое изменяется количество входных каналов на 4. Сама нейросеть уже предобучена на датасете ImageNet. Остальные слои остаются без изменеий. Нейросеть выполняет извлечение признаков из входного изображения, формирует карты признаков. Затем карты передаются в Feature Pyramid Network (FPN), которая объединяет информацию разных размеров и формирует набор признаков? то етсь Regions of Interest. Получается тензор признаков одинакового размера. Далее признаки подаются в Faster R-CNN в блок детекции. Где с помощью полносвязнный слоев анализирует признаки, классифицирует объект в выбранной области и уточняет координаны bounding boxex.
На выходах FPN применяется Region Proposal Network (RPN), предназначенная для генерации кандидатов областей, потенциально содержащих объекты. Дальнейшие этапы Faster R-CNN включают уточнение регионов и классификацию объектов.
FPN формирует карты признаков (набор), что позволяет повысить эффективность детекции объектов разных размеров. Далее применяется RPN (Region Proposal Network). 

https://github.com/user-attachments/assets/2214ab7a-4a11-4f6e-846e-50de80d9b9f4

![base](https://github.com/user-attachments/assets/89045e41-422e-44f1-a3ec-d653f53b55cc)

<img width="2497" height="882" alt="image" src="https://github.com/user-attachments/assets/d3df0f48-c2b0-4648-9335-5fa3ebb43f32" />

#### Модель 2: 4-х канальная модель с механизмом гейтирования (gating_model)
На первом этапе модификации базовой модели вместо слияния был добавлен механизм gating, принцип которого заключается в адаптивном взвешивании признаков rgb и реальной глубины. В модуле gating используется обучаемое значение - гейт, который, в зависимости от сцены, может усиливать или ослаблять вклад одной из двух модальностей, rgb или глубины.

<img width="2530" height="875" alt="image" src="https://github.com/user-attachments/assets/707eb262-4bff-4a83-9216-42a6b6bda84f" />

![gating_model 1](https://github.com/user-attachments/assets/c59eed18-97b4-4d09-af87-42685157af05)

#### Модель 3: 5-ти канальная модель с механизмом гейтирования (rgbdd_gating_model)
Второй этап модификации добавляет в архитектуру пятый канал данных - оцененную глубину, синтезированную с помощью нейросети Midas. Смысл добавления этих данных заключается в том, что реальная глубина может быть сильно зашумлена или же пропускать объекты из-за их дальнего расположения от камеры, или при наличии в сценах светоотражающих или светопропускающих поверхностей. В таких случаях оцененная глубина сможет заменить реальную, а также очертить границы объектов. 

<img width="2890" height="885" alt="image" src="https://github.com/user-attachments/assets/3bf6dbe4-0b99-4ddb-9019-449e512796bd" />

![rgbdd_gating_model 1](https://github.com/user-attachments/assets/0f8303dd-b8de-4c1e-be0b-ea79653617e7)

#### Модель 4: 5-ти канальная модель с механизмом внимания (rgbdd_attention_model)
В третьем этапе модификации был реализован механизм attention, который, в отличии от gating, формирует зависимость между весами rgb, реальной глубины и оцененной глубины. Что позволит модели выбирать наиболее информативный источник данных, что поможет детекции в сценах с сильными перекрытиями(окклюзиями) объектов.

<img width="2882" height="870" alt="image" src="https://github.com/user-attachments/assets/3ae61d62-4641-4bb6-86c5-1c4b5f90a517" />

![rgbdd_attention 1](https://github.com/user-attachments/assets/9037e8d4-7cb1-48a6-a428-2532208d002d)

В процессе экспериментов обучение моделей производилось на 30 эпохах.

### Описание используемых метрик

Ниже приведены суммарные показатели качества обнаружения для четырех моделей:
*базовой*, модели с *gating*, модели с *gating + оценкой глубины* и модели с *Cross-Attention +
Depth*. Показаны стандартные метрики: Precision (точность), Recall (полнота), F1-score и средний
IoU (пересечение по объединению) для обнаруженных объектов:

| Модель                         | Precision | Recall | F1-score | Avg IoU |
|--------------------------------|-----------|--------|----------|---------|
| Base model (IoU ≥ 0.10)         | **0.5262**    | **0.9820** | **0.6853**   | **0.8235**  |
| Gating model                   | 0.7487    | 0.7544 | 0.7515   | 0.8718  |
| Gating + Depth Assessment      | 0.7865    | 0.7450 | **0.7652**   | 0.8714  |
| Cross-Attention + Depth        | 0.7494    | 0.7638 | 0.7566   | 0.8697  |

**Base model.**
Базовая модель обеспечивает почти максимальную полноту обнаружения ```(Recall ≈ 98%)```, но страдает от крайне низкой точности и большого числа ложных срабатываний. Низкий ```IoU``` указывает на грубую локализацию объектов. Она подходит лишь как первичный детектор, если дальнейшая фильтрация ошибок предусмотрена.

**Gating model.**
Gating-модель достигает сбалансированного соотношения точности и полноты ```(около 75%)```, существенно снижая количество ложных тревог по сравнению с базовой моделью. Это даёт заметный рост ```F1-меры``` при относительно простой архитектуре. Модель подходит для универсальных сценариев без использования данных глубины.

**Gating + Depth Assessment.**
Модель с оценкой глубины демонстрирует наивысшие значения ```Precision, F1 и IoU```, обеспечивая наиболее точную детекцию объектов. При этом Recall остаётся высоким и лишь незначительно уступает более простым решениям. В большинстве практических задач она является оптимальным выбором.

**Cross-Attention + Depth.**
Использование ```cross-attention``` позволяет улучшить обнаружение отдельных сложных объектов и немного повысить полноту. Однако это достигается ценой снижения точности и увеличения вычислительной сложности. Модель оправдана в сценариях, где важно максимальное обнаружение трудных случаев.


### Описание приложения
Для проверки работы моделей в режиме реального времени было раработано приложение (инструкция по его запуску представлена в разделе "Устанока и развертывание").

Данное приложение использует библиотеку pyrealsense2 и считывает изображение (rgb + глубина) с камеры RealSense и далее передает на одну из моделей выбранных в конфигурационном файле. Если работа производится для моделей 3 и 4, то предварительно с помощью модели MiDaS по rgb изображению формируется 5 канал информации - оцененная глубина. Она тоже передается в модель.

Для оценки скорости обработки моделей расчитывается время обработки 1 кадра.

В конце приложение визуализирует полученнное rgb избражение с детектируемыми на нем объектами. Так же визуализируются карты глубины (одна для моделей 1 и 2, две для моделей 3 и 4).

## Демонстрация работы

*TODO*

ссылка на видео

gif анимация

## Устанока и развертывание

Обучение моделей производилось с использованием вычислительных мощностей сред Google Colab и Kaggle. Используемые скрипты представлены в папке notebooks.

Приложение для тестирования работы моделей на данных с видеокамеры RealSense разрабатывалось с использованием окружения Anaconda. Файл с используемым окружением: environment.yaml.

Для развертки окружения выполните следующие команды:
- Убедитесь, что у вас установлен Anaconda или Miniconda.
- Создайте окружение на основе файла environment.yml: 
```bash
conda env create -f environment.yml
```
- Активируйте окружение:
```bash
conda activate anaconda_env  # или имя, указанное в поле name в environment.yml
```
- Если зависимости обновлялись, синхронизируйте окружение:
```bash
conda env update --file environment.yml --prune
```

## Запуск и использование

### Обучение моделей
Для повторения процесса обучения моделей необходимо запустить скрипты представленые в папке notebooks. Их можно скачать из репозитория и вреучную выгрузить в Google Colab или Kaggle. Рекомендуется использовать GPU для более быстрого обучения.

Так же скрипты можно запустить локально, но для этого требуются минимум 7.28 Гб свободной памяти для датасета и достаточные вычислительные мощности.

### Конфигурация приложения
Для конфигурации работы приложения необходимо заполнить файл config/config.py.
Необходимо указать значение следующим переменным:
- CLASSES_PATH - путь к файлу с описанием классов,
- MODEL - название используемой модели,
- MODEL_PATH - путь к весам обученной модели,
- DEPTH_MIN, DEPTH_MAX, IMG_SIZE - параметры обучения.

### Запуск приложения для тестирования работы моделей на данных с видеокамеры RealSense
- загрузка репозитория:
```bash
git@github.com:KuranovaPolina/multimodal_object_detection.git
```
- Активация окружения: 
```bash
conda activate anaconda_env
```
- Запуск: 
```bash
python main_test.py
```

## Описание полученных результатов

*TODO*

------- Сравнение метрик в таблице

