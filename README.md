# Исследование мультимодального обнаружения объектов в условиях сильных перекрытий

## Участники 

- Куранова Полина - 466426
- Сегень Радмила (https://github.com/Radmilase) - 472460
- Дерина Вероника (https://github.com/VerDddD) - 465747
- Талканов Амир (https://github.com/BegomAmir) - 412420

## Описание исследования
В ходе выполнения курсовой работы по дисциплине "Методы машинного обучения в робототехнике" было выполнено исследование по теме мультимодального обнаружения объектов в условиях сильных перекрытий.

### Цель исследования
Провести сравнительный анализ 4 моделей для мультимодального обнаружения объектов в условиях сильных перекрытий.

Задача мультимодальной детекции объектов подразумевает под собой детекцию объектов на основе информации имеющей разную природу. В нашем случе используется rgb изображение и карта глубины с камеры, а так же в 2 случаях оцененная карта глубины полученная с помощью внешней модели [MiDaS](https://github.com/isl-org/MiDaS).

### Гипотеза
Модель с механизмом внимания и дополнительным каналом синтезированной глубины должна демонстрировать наилучшее качество детекции по сравнению с альтернативной моделью с механизмом gating и более простыми версиями.

### Задачи исследования
- Снять датасет (определить детектируемые объекты, объем датасета и снимаемые сцены).
- Разметить датасет (использовалась платформа Label Studio).
- Реализовать модели.
- Обучить модели, рассчитать метрики качества обучения модели.
- Реализовать приложение для тестирования работы в режиме реального времени.
- Протестировать приложение для каждой модели, рассчитать среднее время обработки кадра для каждой модели.

### Описание репозитория

В данном репозитории представлен исходный код используемый для проведения исследования.

В папке notebooks представлены скрипты ипользуемые для обучения моделей, а в папке src представлен исходный код необходимый для работы приложения. Главный файл приложения [main_test.py](https://github.com/KuranovaPolina/multimodal_object_detection/blob/main/main_test.py).

### Описание датасета
Данный датасет был собран и размечен вручную. Он включает 3192 пары данных, каждая из которых состоит из RGB-изображения и соответствующей карты глубины, полученных синхронно. Съёмка данных выполнялась с использованием RGB-D камеры Intel RealSense.
Объекты снимались в условиях частичных перекрытий объектов с наличием отражающих поверхностей. Объекты располагались в произвольных положениях и ориентациях. Во большинстве сценах более двух объектов одновременно, в том числе частично перекрывающихся между собой либо другими предметами.

#### Объекты и классы
Были взяты как простые объекты, так и сложный: шары различных цветов; кольцевые объекты различного размера и цвета; бутылка железная; бутылка пластиквая; деревянные кубики; игрушечная уточка; ножницы; скотч.
Каждый объект отнесён к одному из следующих классов: bottle, cube, duck, scissors, sphere, tape, tor.

#### Разметка данных
Разметка датасета выполнялась вручную при помощи [Label Studio](https://labelstud.io/). Каждый целевой объект выделялся в виде bounding box и ему присваивалась метка класса. Формат разметки совместим с Faster R-CNN и YOLO.

ПАРА КАРТИНОК

### Инструкция по установке датсета
Датасет на котором обучались модели находится по [ссылке](https://www.kaggle.com/datasets/radmilasegen/dataset-fin).
Для его установки используйте следующие команды:

```python
import kagglehub

# Download latest version
path = kagglehub.dataset_download("radmilasegen/dataset-fin")

print("Path to dataset files:", path)
```
В файле [merged_classes.txt](merged_classes.txt) представлен список детектируемых объектов. Этот же файл можно найти в датасете.


### Описание моделей
В проекте представлен путь модификаций четырёхканальной RGB-D модели с простейшим слиянием, основанной на статье [Multimodal Object Detection using Depth and Image Data for Manufacturing Parts](https://www.researchgate.net/publication/385823214_Multimodal_Object_Detection_using_Depth_and_Image_Data_for_Manufacturing_Parts)
, в дальнейшем базовая модель.

#### Модель 1: Базовая модель (RGB-D Faster R-CNN с ранним слиянием входных данных и ResNet-50 + FPN backbone)
Модель основана на [Faster R-CNN](https://arxiv.org/abs/1506.01497) с backbone от ResNet50, а затем расширяется с помощью FPN (Feature Pyramid Network).
На вход подается цветное изображение с 3-мя каналами RGB и 1 канал глубины Depth, преварительно нормализованный. Далее происходит ранее слияние (early fusion), то есть конкатенация каналов. Получившийся 4-х канальный тензор подается на ResNet50. Для обработки RGB-D в первой слое изменяется количество входных каналов на 4. Сама нейросеть уже предобучена на датасете ImageNet. Остальные слои остаются без изменеий. Нейросеть выполняет извлечение признаков из входного изображения, формирует карты признаков. Затем карты передаются в Feature Pyramid Network (FPN), которая объединяет информацию разных размеров и формирует набор признаков. На выходах FPN применяется Region Proposal Network (RPN), предназначенная для генерации кандидатов областей, потенциально содержащих объекты. Дальнейшие этапы Faster R-CNN включают уточнение регионов и классификацию объектов. FPN формирует карты признаков (набор), что позволяет повысить эффективность детекции объектов разных размеров. Далее применяется RPN (Region Proposal Network). Получается тензор признаков одинакового размера. Далее признаки подаются в Faster R-CNN в блок детекции. Где с помощью полносвязнный слоев анализирует признаки, классифицирует объект в выбранной области и уточняет координаны bounding boxex.
 
<img width="3527" height="760" alt="image" src="https://github.com/user-attachments/assets/9a44119c-0387-43c7-be32-2ec455fee9da" />


![base1](https://github.com/user-attachments/assets/f926534f-cbc3-4cdc-95ea-e31d2c0a790f)


#### Модель 2: 4-х канальная модель с механизмом гейтирования (gating_model)

В данной архитектуре используется **CNN-stem** - начальный свёрточный блок нейронной сети, предназначенный для преобразования входных данных в первичное скрытое представление, пригодное для дальнейшей обработки основным backbone.

Stem в свёрточных нейронных сетях выполняет роль *адаптера* между пространством входных данных (например, RGB-изображением) и пространством иерархических признаков, с которыми работает CNN.

CNN-stem решает несколько ключевых задач:

1. **Преобразование входа в пространство признаков**  
   Stem переводит входной тензор размера $H \times W \times C$ в тензор признаков с большим числом каналов, которые используются последующими свёрточными блоками. Это задаёт стартовую размерность признаков всей модели.

2. **Извлечение низкоуровневых признаков**  
   На уровне stem сеть учится выделять базовые визуальные структуры: границы, локальные контрасты, текстуры и простые паттерны. Эти признаки являются фундаментом для построения более абстрактных представлений на глубоких слоях.

3. **Ранняя стабилизация обучения**  
   Использование нормализации (BatchNorm / GroupNorm) и нелинейностей в stem снижает чувствительность модели к масштабам входных данных и улучшает сходимость обучения.

4. **Контролируемое понижение пространственного разрешения**  
   Часто stem выполняет downsampling, что уменьшает вычислительную нагрузку последующих слоёв и увеличивает эффективное receptive field сети.

В общем виде CNN-stem представляет собой последовательность операций:

$$
X_0 = \phi\bigl(\mathrm{Norm}(\mathrm{Conv}(I))\bigr)
$$

где: $I$ — входной тензор (например, RGB-изображение), $\mathrm{Conv}(\cdot)$ — свёрточная операция (обычно с ядром $3\times3$ или $7\times7$), $\mathrm{Norm}(\cdot)$ — операция нормализации (BatchNorm или GroupNorm), $\phi(\cdot)$ — нелинейная функция активации (ReLU, SiLU и др.), $X_0$ — выходной тензор stem, первый уровень признаков сети.


На практике stem может состоять из нескольких последовательно применяемых свёрток $3 \times 3$, что позволяет увеличить выразительность за счёт дополнительных нелинейностей, избежать чрезмерно агрессивной свёртки большого размера, аккуратнее сохранить локальные детали.

Stem определяет начальный баланс между детализацией и инвариантность, слишком агрессивный stem (большой stride или крупное ядро) может разрушить мелкие структуры, слишком слабый stem увеличивает вычислительную стоимость и замедляет обучение, выбор числа выходных каналов влияет на выразительность всей сети.

Gating — это класс дифференцируемых механизмов управления потоками информации в нейронных сетях, реализующий адаптивное дозирование вкладов отдельных признаков, каналов или модальностей в итоговое представление.
В отличие от жёстких логических переключателей, gating реализуется в непрерывной форме и обучается совместно с основной моделью с помощью градиентной оптимизации.

Основная идея gating состоит в том, что не вся доступная информация должна использоваться одинаково интенсивно во всех контекстах: сеть должна уметь условно пропускать, ослаблять или подавлять отдельные сигналы в зависимости от их полезности для текущей задачи.

Формула, приведенная ниже, реализует механизм gating (вентильного управления) при слиянии двух представлений признаков.

$$
F_{\text{fused}} = \alpha \cdot F_{\text{att}} + (1 - \alpha) \cdot F_{\text{rgb}}
$$

- **$F_{\text{fused}}$** — итоговое (fused) представление признаков, используемое на последующих этапах модели.

- **$F_{\text{att}}$** — *кандидатное, усиленное представление*, как правило полученное после attention-модуля.  
  Данный тензор содержит признаки с повышенной селективностью, однако может быть чувствителен к шуму или ошибочным активациям.

- **$F_{\text{rgb}}$** — *базовое (опорное) представление* признаков, извлечённое напрямую из основной ветви без дополнительного усиления.  
  Рассматривается как более стабильный и надёжный источник информации.

- **$\alpha$** — **gate-переменная (вентиль)**, определяющая степень доверия к attention-усиленному представлению.  
  Значение $\alpha$ предсказывается параметризованным модулем и ограничено интервалом $[0, 1]$.

Таким образом, формула реализует gating-механизм, позволяющий адаптивно регулировать влияние усиленных признаков в зависимости от контекста входных данных.

<img width="3727" height="705" alt="image" src="https://github.com/user-attachments/assets/27bdab4b-45d6-48fe-993e-6d28050ebf16" />

![gating_model 1](https://github.com/user-attachments/assets/c59eed18-97b4-4d09-af87-42685157af05)

#### Модель 3: 5-ти канальная модель с механизмом гейтирования (rgbdd_gating_model)
Второй этап модификации добавляет в архитектуру пятый канал данных - оцененную глубину, синтезированную с помощью нейросети Midas. Смысл добавления этих данных заключается в том, что реальная глубина может быть сильно зашумлена или же пропускать объекты из-за их дальнего расположения от камеры, или при наличии в сценах светоотражающих или светопропускающих поверхностей. В таких случаях оцененная глубина сможет заменить реальную, а также очертить границы объектов. 

<img width="3745" height="660" alt="image" src="https://github.com/user-attachments/assets/74bff23d-aadd-4a54-84b5-a193ed79d742" />


![rgbdd_gating_model 1](https://github.com/user-attachments/assets/0f8303dd-b8de-4c1e-be0b-ea79653617e7)

#### Модель 4: 5-ти канальная модель с механизмом внимания (rgbdd_attention_model)
В данной архитектуре модуль Cross-Modal Attention (CMA) используется для объединения признаков из нескольких модальностей: RGB, карты глубины и оценённой глубины. CMA располагается после отдельных stem-блоков для каждой модальности и перед общим бэкбоном ResNet-50.

Основная задача CMA — умно объединять модальности, а не просто складывать или конкатенировать их признаки. Модуль анализирует содержимое признаков и динамически определяет, какой вклад каждая модальность должна вносить в итоговое представление. Благодаря этому сеть может усиливать полезную геометрическую информацию из глубины и одновременно снижать влияние шума, например пропусков в depth-данных или ошибок монокулярной оценки глубины.

CMA реализуется через механизм кросс-внимания: признаки одной модальности используются для выборки релевантной информации из других модальностей. Полученные межмодальные признаки объединяются с исходными с помощью residual-соединений, формируя единый тензор признаков, который затем подаётся на стандартный пайплайн Faster R-CNN (ResNet-50, FPN, RPN, ROI Align и детекторную голову).

<img width="3782" height="610" alt="image" src="https://github.com/user-attachments/assets/a1576738-52e8-4989-80fa-61326d0ae671" />

![rgbdd_attention 1](https://github.com/user-attachments/assets/9037e8d4-7cb1-48a6-a428-2532208d002d)

В процессе экспериментов обучение моделей производилось на 30 эпохах.

### Описание используемых метрик
В данном исследовании были использованы следующие метрики качества моделей:

**Precision** характеризует долю корректных положительных предсказаний среди всех предсказанных положительных объектов и вычисляется по формуле:

$$
Precision = \frac{TP}{TP + FP}
$$

где $TP$ — число истинно положительных предсказаний, а $FP$ — число ложно положительных предсказаний.

**Recall** (полнота) отражает способность модели обнаруживать реальные положительные объекты и определяется как:

$$
Recall = \frac{TP}{TP + FN}
$$

где $FN$ — число ложно отрицательных предсказаний.

**F1-score** представляет собой гармоническое среднее Precision и Recall и используется для сбалансированной оценки качества модели, особенно при наличии дисбаланса классов:

$$
F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$

**Average IoU (Intersection over Union)** оценивает степень пространственного совпадения между предсказанными и истинными областями и вычисляется как:

$$
IoU = \frac{|A \cap B|}{|A \cup B|}
$$

где $A$ — предсказанная область, а $B$ — эталонная разметка.  
Average IoU определяется как среднее значение $IoU$ по всем объектам.



### Описание приложения
Для проверки работы моделей в режиме реального времени было раработано приложение (инструкция по его запуску представлена в разделе "Устанока и развертывание").

Данное приложение использует библиотеку pyrealsense2 и считывает изображение (rgb + глубина) с камеры RealSense и далее передает на одну из моделей выбранных в конфигурационном файле. Если работа производится для моделей 3 и 4, то предварительно с помощью модели MiDaS по rgb изображению формируется 5 канал информации - оцененная глубина. Она тоже передается в модель.

Для оценки скорости обработки моделей расчитывается время обработки 1 кадра.

В конце приложение визуализирует полученнное rgb избражение с детектируемыми на нем объектами. Так же визуализируются карты глубины (одна для моделей 1 и 2, две для моделей 3 и 4).

## Демонстрация работы

Модель 1:

![base1](https://github.com/user-attachments/assets/699ea9be-5d76-4a70-9690-c0a819b14ae4)

Модель 2:

![gating_model 5](https://github.com/user-attachments/assets/c8edcb27-134d-4684-83ea-8f71ce5f2899)

Модель 3:

![rgbdd_gating_model 2](https://github.com/user-attachments/assets/0eeda377-f3ad-4c70-b167-24a91651bfb2)

Модель 4:

![rgbdd_attention 2](https://github.com/user-attachments/assets/d8b81f8e-9eda-4d87-b257-546c570c4262)

## Установка и развертывание

Обучение моделей производилось с использованием вычислительных мощностей сред Google Colab и Kaggle. Используемые скрипты представлены в папке notebooks.

Приложение для тестирования работы моделей на данных с видеокамеры RealSense разрабатывалось с использованием окружения Anaconda. Файл с используемым окружением: environment.yaml.

Для развертки окружения выполните следующие команды:
- Убедитесь, что у вас установлен Anaconda или Miniconda.
- Создайте окружение на основе файла environment.yml: 
```bash
conda env create -f environment.yml
```
- Активируйте окружение:
```bash
conda activate anaconda_env  # или имя, указанное в поле name в environment.yml
```
- Если зависимости обновлялись, синхронизируйте окружение:
```bash
conda env update --file environment.yml --prune
```

## Запуск и использование

### Обучение моделей
Для повторения процесса обучения моделей необходимо запустить скрипты представленые в папке notebooks. Их можно скачать из репозитория и вреучную выгрузить в Google Colab или Kaggle. Рекомендуется использовать GPU для более быстрого обучения.

Так же скрипты можно запустить локально, но для этого требуются минимум 7.28 Гб свободной памяти для датасета и достаточные вычислительные мощности.

### Конфигурация приложения
Для конфигурации работы приложения необходимо заполнить файл config/config.py.
Необходимо указать значение следующим переменным:
- CLASSES_PATH - путь к файлу с описанием классов,
- MODEL - название используемой модели,
- MODEL_PATH - путь к весам обученной модели,
- DEPTH_MIN, DEPTH_MAX, IMG_SIZE - параметры обучения.

### Запуск приложения для тестирования работы моделей на данных с видеокамеры RealSense
- загрузка репозитория:
```bash
git@github.com:KuranovaPolina/multimodal_object_detection.git
```
- Активация окружения: 
```bash
conda activate anaconda_env
```
- Запуск: 
```bash
python main_test.py
```

## Описание полученных результатов

Ниже приведены суммарные показатели качества обнаружения для четырех моделей:
*базовой*, модели с *gating*, модели с *gating + оценкой глубины* и модели с *Cross-Attention +
Depth*. Показаны стандартные метрики: Precision (точность), Recall (полнота), F1-score и средний
IoU (пересечение по объединению) для обнаруженных объектов:

| Модель                         | Precision | Recall | F1-score | Avg IoU |
|--------------------------------|-----------|--------|----------|---------|
| Base model (IoU ≥ 0.10)         | **0.5262**    | **0.9820** | **0.6853**   | **0.8235**  |
| Gating model                   | 0.7487    | 0.7544 | 0.7515   | 0.8718  |
| Gating + Depth Assessment      | 0.7865    | 0.7450 | **0.7652**   | 0.8714  |
| Cross-Attention + Depth        | 0.7494    | 0.7638 | 0.7566   | 0.8697  |

**Base model.**
Базовая модель обеспечивает почти максимальную полноту обнаружения ```(Recall ≈ 98%)```, но страдает от крайне низкой точности и большого числа ложных срабатываний. Низкий ```IoU``` указывает на грубую локализацию объектов. Она подходит лишь как первичный детектор, если дальнейшая фильтрация ошибок предусмотрена.

**Gating model.**
Gating-модель достигает сбалансированного соотношения точности и полноты ```(около 75%)```, существенно снижая количество ложных тревог по сравнению с базовой моделью. Это даёт заметный рост ```F1-меры``` при относительно простой архитектуре. Модель подходит для универсальных сценариев без использования данных глубины.

**Gating + Depth Assessment.**
Модель с оценкой глубины демонстрирует наивысшие значения ```Precision, F1 и IoU```, обеспечивая наиболее точную детекцию объектов. При этом Recall остаётся высоким и лишь незначительно уступает более простым решениям. В большинстве практических задач она является оптимальным выбором.

**Cross-Attention + Depth.**
Использование ```cross-attention``` позволяет улучшить обнаружение отдельных сложных объектов и немного повысить полноту. Однако это достигается ценой снижения точности и увеличения вычислительной сложности. Модель оправдана в сценариях, где важно максимальное обнаружение трудных случаев.
